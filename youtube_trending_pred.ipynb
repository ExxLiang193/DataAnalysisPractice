{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "youtube_trending_pred.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMV6bP7JgeutyF6iOU8dgo2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ExxLiang193/DataAnalysisPractice/blob/master/youtube_trending_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsZUovNV1O-i",
        "colab_type": "text"
      },
      "source": [
        "Developed through Google Colab.\n",
        "\n",
        "Please download dataset from https://www.kaggle.com/datasnaek/youtube-new/data#CAvideos.csv before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk52wN6DZz2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-gBX_ZQZ32n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh3h8OUYZ56r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPyRdcoBCwMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from time import perf_counter\n",
        "from functools import wraps, partial\n",
        "import  pyspark.sql.functions as F\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# warnings.filterwarnings(action='once')\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0MDGyRCKpQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_category_data():\n",
        "    # Load raw category data on music\n",
        "    with open('CA_category_id.json', 'r') as data:\n",
        "        ca_cat_info = json.load(data)\n",
        "\n",
        "    # Parse raw category data into ids, title, and assignable status\n",
        "    with open('CA_category_id.csv', 'w') as outf:\n",
        "        for cat in ca_cat_info['items']:\n",
        "            outf.write(','.join([cat['id'], cat['snippet']['title'],\n",
        "                                str(cat['snippet']['assignable'])]) + '\\n')\n",
        "parse_category_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYLsqNBEzQG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def measure_time(f):\n",
        "    @wraps(f)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        t0 = perf_counter()\n",
        "        res = f(*args, **kwargs)\n",
        "        print(\"--{}-- process took: {} sec\".format(f.__name__,\n",
        "                                                   round(perf_counter() - t0, 5)))\n",
        "        return res\n",
        "    return wrapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-ACSK4YQWPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TIME_FEATURE_COLS = [\n",
        "    'publish_hour', 'publish_weekday', 'days_before_trending', 'trending_day_of_year'\n",
        "]\n",
        "\n",
        "def time_features(row):\n",
        "    publish_datetime = pd.to_datetime(row['publish_time'])\n",
        "    trending_date = pd.to_datetime(row['trending_date'], format='%y.%d.%m')\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate publish hour of day.\n",
        "    \"\"\"\n",
        "    row['publish_hour'] = row['publish_time'].timetuple().tm_hour\n",
        "\n",
        "    row['publish_weekday'] = row['publish_time'].timetuple().tm_wday\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate number of days before on trending.\n",
        "    \"\"\"\n",
        "    row['days_before_trending'] = (trending_date.date() - publish_datetime.date()).days\n",
        "    \n",
        "    \"\"\"\n",
        "    Convert trending date to day of year.\n",
        "    \"\"\"\n",
        "    row['trending_day_of_year'] = trending_date.timetuple().tm_yday\n",
        "    \n",
        "    return row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU5gcSUkJZta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_FEATURE_COLS = [\n",
        "    'title_length', 'desc_length', 'title_cap_ratio',\n",
        "    'title_sep_count', 'title_special_char_count',\n",
        "    'desc_url_count', 'sub_request', 'desc_cap_ratio',\n",
        "    'mentions_music', 'mentions_game', 'tag_count'\n",
        "]\n",
        "\n",
        "SEPARATORS = ['|', '/', '-']\n",
        "SEP_CAP = 8\n",
        "SPECIAL_CHARS = ['?', '!']\n",
        "SPECIAL_CHARS_CAP = 10\n",
        "MAX_COMMON = 200\n",
        "\n",
        "def char_counter(phrase, chars, limiter):\n",
        "    count = 0\n",
        "    for c in phrase:\n",
        "        if c in chars:\n",
        "            count += 1\n",
        "    return min(count, limiter) if limiter else count\n",
        "\n",
        "\n",
        "def text_features(row):\n",
        "    \"\"\"\n",
        "    Calculate title length.\n",
        "    \"\"\"\n",
        "    row['title_length'] = len(row['title'])\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate description length.\n",
        "    \"\"\"\n",
        "    row['desc_length'] = len(row['description'] or '')\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate proportion of uppercase letters in title.\n",
        "    \"\"\"\n",
        "    row['title_cap_ratio'] = \\\n",
        "        sum(1 for c in row['title'] if c.isupper()) / len(row['title'])\n",
        "\n",
        "    \"\"\"\n",
        "    Count number of separators (|, /, -). Generally,\n",
        "    separators are used to separate important contributors,\n",
        "    thus affecting click rate.\n",
        "    \"\"\"\n",
        "    row['title_sep_count'] = \\\n",
        "        char_counter(row['title'], chars=SEPARATORS, limiter=SEP_CAP)\n",
        "\n",
        "    \"\"\"\n",
        "    Count number of special characters (!,?)\n",
        "    \"\"\"\n",
        "    row['title_special_char_count'] = \\\n",
        "        char_counter(row['title'], chars=SPECIAL_CHARS,\n",
        "                     limiter=SPECIAL_CHARS_CAP)\n",
        "\n",
        "    \"\"\"\n",
        "    Count number of urls in description of video. We can assume that\n",
        "    having 'http' is one occurrence of a url.\n",
        "    \"\"\"\n",
        "    row['desc_url_count'] = row['description'].count('http')\n",
        "\n",
        "    \"\"\"\n",
        "    Creator's video's description requested to subscribe.\n",
        "    \"\"\"\n",
        "    row['sub_request'] = int('subscribe' in row['description'].lower())\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate proportion of uppercase letters in description.\n",
        "    \"\"\"\n",
        "    if row['description']:\n",
        "        row['desc_cap_ratio'] = \\\n",
        "            sum(1 for c in row['description'] if c.isupper()) \\\n",
        "            / len(row['description'])\n",
        "    else:\n",
        "        row['desc_cap_ratio'] = 0.0\n",
        "\n",
        "    \"\"\"\n",
        "    Video's description or title mentions music.\n",
        "    \"\"\"\n",
        "    row['mentions_music'] = int('music' in row['title'].lower() or\n",
        "                                'music' in row['description'].lower())\n",
        "\n",
        "    \"\"\"\n",
        "    Video's description or title mentions gaming.\n",
        "    \"\"\"\n",
        "    row['mentions_game'] = int('game' in row['title'].lower() or\n",
        "                               'game' in row['description'].lower())\n",
        "\n",
        "    \"\"\"\n",
        "    Get number of tags in video.\n",
        "    \"\"\"\n",
        "    row['tag_count'] = row['tags'].count('|') + 1\n",
        "\n",
        "    return row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5IY62-zuGMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOOL_COLS = ['comments_disabled', 'ratings_disabled', 'video_error_or_removed']\n",
        "\n",
        "def bool_features(row):\n",
        "    \"\"\"\n",
        "    Transform boolean columns into binary variables.\n",
        "    \"\"\"\n",
        "    for col in BOOL_COLS:\n",
        "        row[col] = int(row[col] == 'True')\n",
        "    return row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34WA3DByd7cN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DROP_COLS = ['video_id', 'thumbnail_link']\n",
        "\n",
        "@measure_time\n",
        "def preprocess(data):\n",
        "    data = data.withColumnRenamed('description\\r', 'description')\n",
        "    data = data.fillna('', subset=['tags', 'description'])\n",
        "\n",
        "    # Drop columns that don't provide useful info\n",
        "    return data.drop(*DROP_COLS)\n",
        "\n",
        "\n",
        "FEATURE_MAPPERS = [\n",
        "    time_features, text_features, bool_features\n",
        "]\n",
        "\n",
        "def row_mapper(row):\n",
        "    row = row.asDict()\n",
        "    for mapper in FEATURE_MAPPERS:\n",
        "        row = mapper(row)\n",
        "    return tuple(row[key] for key in FEATURE_MAPPED_COLS)\n",
        "\n",
        "@measure_time\n",
        "def map_features(data):\n",
        "    return data.rdd.map(row_mapper).toDF(FEATURE_MAPPED_COLS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H47LKCfH4TBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_TAG_RANK = 20\n",
        "INDEX = ['channel_title', 'title']\n",
        "\n",
        "\n",
        "def reduce_multilevel_cols(cols):\n",
        "    return cols.map('_'.join).str.strip('_')\n",
        "\n",
        "\n",
        "@measure_time\n",
        "def aggregate_features(pdata):\n",
        "    \"\"\"\n",
        "    Get average tag rank based on aggregation of tags across dataset.\n",
        "    \"\"\"\n",
        "    tag_counter = Counter()\n",
        "    tag_only_df = pdata[INDEX + ['tags']].drop_duplicates()\n",
        "    tag_only_df['tags'] = tag_only_df['tags'].map(\n",
        "        lambda tags: [tag.strip('\"') for tag in tags.split('|')]\n",
        "    )\n",
        "    for tag_list in tag_only_df['tags']:\n",
        "        for tag in tag_list:\n",
        "            tag_counter[tag] += 1\n",
        "    tag_rank = {pair[0]: index for index, pair in enumerate(tag_counter.most_common(MAX_TAG_RANK))}\n",
        "    tag_only_df['tag_rank'] = tag_only_df['tags'].map(\n",
        "        lambda tags: np.mean([tag_rank[tag] for tag in tags if tag in tag_rank])\n",
        "    )\n",
        "    tag_only_df['tag_rank'].fillna(MAX_TAG_RANK, inplace=True)\n",
        "    tag_only_df.drop(['tags'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "    pdata['creator_videos_trending'] = 1\n",
        "    agg_features = pd.pivot_table(pdata, index=INDEX,\n",
        "                                  aggfunc={'creator_videos_trending': len,\n",
        "                                           'trending_day_of_year': [min, max],\n",
        "                                           'days_before_trending': [min, max],\n",
        "                                           'views': [min, max],\n",
        "                                           'likes': [min, max],\n",
        "                                           'dislikes': [min, max],\n",
        "                                           'comment_count': [min, max]})\n",
        "    agg_features.columns = reduce_multilevel_cols(agg_features.columns)\n",
        "\n",
        "    agg_features['days_on_trending'] = \\\n",
        "        agg_features['days_before_trending_max'] - agg_features['days_before_trending_min'] + 1\n",
        "\n",
        "    agg_features['views_growth'] = agg_features['views_max'] - agg_features['views_min']\n",
        "    agg_features['likes_growth'] = agg_features['likes_max'] - agg_features['likes_min']\n",
        "    agg_features['dislikes_growth'] = agg_features['dislikes_max'] - agg_features['dislikes_min']\n",
        "    agg_features['comments_growth'] = \\\n",
        "        agg_features['comment_count_max'] - agg_features['comment_count_min']\n",
        "\n",
        "\n",
        "    agg_features.drop(['days_before_trending_max', 'views_max',\n",
        "                       'likes_max', 'dislikes_max', 'comment_count_max'], axis=1, inplace=True)\n",
        "    agg_features.rename(columns={'days_before_trending_min': 'days_before_trending',\n",
        "                                 'creator_videos_on_trending_len': 'creator_videos_trending',\n",
        "                                 'views_min': 'init_views',\n",
        "                                 'likes_min': 'init_likes',\n",
        "                                 'dislikes_min': 'init_dislikes',\n",
        "                                 'comment_count_min': 'init_comments'},\n",
        "                        inplace=True)\n",
        "    agg_features.reset_index(inplace=True)\n",
        "\n",
        "    return pd.merge(tag_only_df, agg_features, on=INDEX, how='inner')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06HNcTR_lEJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INDEX = ['channel_title', 'title']\n",
        "\n",
        "def generate_circular_repr(data, category, cycle_length):\n",
        "    s = list(data[category].map(\n",
        "        lambda entry: [np.sin(entry * np.pi / (cycle_length / 2.)),\n",
        "                       np.cos(entry * np.pi / (cycle_length / 2.))]\n",
        "    ))\n",
        "    data[[category + '_x', category + '_y']] = pd.DataFrame(s)\n",
        "    data.drop(category, axis=1, inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "@measure_time\n",
        "def post_processing(agg_features, ca_data):\n",
        "    encoded_categories = pd.get_dummies(ca_data['category_id'])\n",
        "    for col in encoded_categories.columns:\n",
        "        ca_data['category_' + str(col)] = encoded_categories[col]\n",
        "\n",
        "    ca_data = ca_data.drop(['views', 'likes', 'dislikes', 'trending_date',\n",
        "                            'comment_count', 'tags', 'category_id', 'description',\n",
        "                            'creator_videos_trending', 'trending_day_of_year',\n",
        "                            'days_before_trending', 'publish_time'], axis=1).drop_duplicates()\n",
        "    \n",
        "    ca_data = pd.merge(ca_data, agg_features, how='inner', on=INDEX)\n",
        "\n",
        "    ca_data = generate_circular_repr(ca_data, 'publish_hour', 24)\n",
        "    ca_data = generate_circular_repr(ca_data, 'publish_weekday', 7)\n",
        "    ca_data = generate_circular_repr(ca_data, 'trending_day_of_year_min', 365)\n",
        "    ca_data = generate_circular_repr(ca_data, 'trending_day_of_year_max', 365)\n",
        "    \n",
        "    return ca_data.drop(INDEX, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cCHnsCjGlX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, FloatType,\n",
        "    DoubleType, IntegerType, BooleanType, DateType, TimestampType)\n",
        "\n",
        "RAW_COLS = {\n",
        "    'video_id': StringType(),\n",
        "    'trending_date': StringType(),\n",
        "    'title': StringType(),\n",
        "    'channel_title': StringType(),\n",
        "    'category_id': IntegerType(),\n",
        "    'publish_time': TimestampType(),\n",
        "    'tags': StringType(),\n",
        "    'views': DoubleType(),\n",
        "    'likes': DoubleType(),\n",
        "    'dislikes': DoubleType(),\n",
        "    'comment_count': IntegerType(),\n",
        "    'thumbnail_link': StringType(),\n",
        "    'comments_disabled': BooleanType(),\n",
        "    'ratings_disabled': BooleanType(),\n",
        "    'video_error_or_removed': BooleanType(),\n",
        "    'description': StringType()\n",
        "}\n",
        "\n",
        "PREPROCESSED_COLS = list(set(RAW_COLS) - set(DROP_COLS))\n",
        "\n",
        "FEATURE_MAPPED_COLS = PREPROCESSED_COLS + TIME_FEATURE_COLS + TEXT_FEATURE_COLS\n",
        "\n",
        "@measure_time\n",
        "def workflow():\n",
        "    # Load raw video trending data from Youtube\n",
        "    @measure_time\n",
        "    def read_raw_data():\n",
        "        return spark.read.load('CAvideos.csv',\n",
        "                               format='com.databricks.spark.csv',\n",
        "                               header=True, multiLine=True,\n",
        "                               schema=StructType([StructField(key, dtype, False)\n",
        "                                                  for key, dtype in RAW_COLS.items()]))\n",
        "\n",
        "    ca_data = read_raw_data()\n",
        "    ca_data = preprocess(ca_data)\n",
        "    ca_data = map_features(ca_data)\n",
        "    ca_data = ca_data.toPandas()\n",
        "    agg_features = aggregate_features(ca_data)\n",
        "    ca_features = post_processing(agg_features, ca_data)\n",
        "    return ca_features\n",
        "\n",
        "\n",
        "result = workflow()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_iXJ26SuYqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABELS = ['views_growth']\n",
        "\n",
        "def training(stuff):\n",
        "    features = stuff.copy()\n",
        "    labels = np.array(features[LABELS])\n",
        "    features.drop(LABELS, axis=1, inplace=True)\n",
        "    feature_list = list(features.columns)\n",
        "    features = np.array(features)\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_features, test_features, train_labels, test_labels = \\\n",
        "        train_test_split(features, labels, test_size = 0.25)\n",
        "    \n",
        "    print('Training Features Shape:', train_features.shape)\n",
        "    print('Training Labels Shape:', train_labels.shape)\n",
        "    print('Testing Features Shape:', test_features.shape)\n",
        "    print('Testing Labels Shape:', test_labels.shape)\n",
        "\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    rf = RandomForestRegressor(n_estimators = 100)\n",
        "    rf.fit(train_features, train_labels)\n",
        "\n",
        "    # Use the forest's predict method on the test data\n",
        "    predictions = rf.predict(test_features)\n",
        "    # Calculate the absolute errors\n",
        "    errors = abs(predictions - test_labels)\n",
        "\n",
        "    # Calculate mean absolute percentage error (MAPE)\n",
        "    mape = 100 * (errors / test_labels)\n",
        "    # Calculate and display accuracy\n",
        "    accuracy = 100 - np.mean(mape)\n",
        "    print('Accuracy:', round(accuracy, 2), '%.')\n",
        "\n",
        "training(result)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}